## 对比32b回复，7b模型存在的问题

1. 7b指令遵循能力相较于32b有差距，容易受到prompt中提供的one-shot例子的影响；

2. analysis部分(思维链)，目前对这部分缺少校验

3. 基于matching_features(提到的电影名称+提到的电影名称的属性+回复中提到的属性) / generate_features的方式确实能够让模型的回复**更简洁**，这样的方式是否好有待商榷（使用**原7b模型**生成偏好总结，匹配模型使用like+dislike的合并文本仍可达到SFT后模型的效果）

4. 7b模型容易忽略conversation中用户表达明确偏好的电影名称，而是将其属性提取出来放入偏好总结，而对click的电影会倾向于直接将电影名称纳入偏好总结，这个行为与32b模型相反（当然，也会出现7b加入的这部click电影刚好和标签电影一样的情况（而32b模型没有加入））

5. 7b模型，尤其是基于matching / generate取top1的数据进行SFT的7b模型，表现出recall@50长尾数据和32b差异大的情况，可能是"生成较少回复"的限制导致的（模型损失了从长交互序列中进行详细的提取的能力）

6. 7b产生数据中，dislike为空的概率相较于32b较少，与提示中提供的示例有关

## 针对SFT数据获取的方法的思考：

1. 基于32b模型进行打分，提供输入prompt和7b的多条7b的回复，以及**标签值**，选择**打分最高**的回复，这样的数据在合理性，推理逻辑，偏好总结上应该都可以达到更好的效果。

2. 在进行方案一改动之前，可进行以下提取SFT数据的改动：针对同一组样本，先选择matching最大的数据，若matching最大的数据有多个，再选择matching / generate最大的数据，先保证覆盖率，再尽可能简化对电影和属性的列举。

## 对于DPO：
### 目前的做法：
1. DPO负例不能在格式等方面就和正例差距过大，目前DPO负例的构造仅在用户历史交互记录上有差别，temperature仍使用1.同时会在获取数据时对User Preference部分的格式进行校验

2. 在去除对话文本时，保留的是User和Recommender提到的电影名称（若仅保留User提到的电影，一方面对应不上我们对对话部分的处理，另一方面，容易导致某样本直接缺少历史交互记录）

3. 这样的正负例是否过于易于区分？可能可以**指导模型多关注conversation部分**，待实验验证:
   DPO输入的prompt是带有对话文本的原交互记录，chosen相较于rejected过于好区分（训练曲线验证）；正例和负例数据格式上应该追求接近

4. 目前通过曲线可以观察到：reward和chosen确实太容易分辨了，可能会导致reward hacking，并且目前基于matching 或者 matching / generate的方法未得到证实
https://github.com/hiyouga/LLaMA-Factory/issues/5484

想到的改进：
1. 可以提高替换对话文本的比例，但不全部转换为click文本。同时混合一部分获得SFT数据阶段得分较低的回复。
2. early stop，这样也可以避免后期可能产生的震荡
3. 仍然引入大模型打分的方法。
4. 可不可以在正例中使用32b的数据，而不完全依赖自蒸馏呢？

5. 考虑到 (prompt, chosen, rejected)的输入形式：
**针对DPO数据的构造，似乎不修改prompt，修改的话容易引入额外偏差**，使用低质量的prompt非常容易让正例**仅相对负例更好**，而非真的优秀
是否可以在正例方面使用32b模型的数据，在负例方面采取7b生成的质量较差样本呢（基于打分 / matching，matching分数较低的应该确实是质量较差的数据）
或者还是基于大模型打分，7b模型在动态温度的情况下生成多条回复增加多样性，然后依照32b的打分得到正负例，这符合自蒸馏的思想

## 思考
微调应该引导**利用好交互历史、正确对待交互历史**，如果仅着眼于标签值及其属性，SFT和DPO阶段相当于忽略了”包含思维链“这个偏好分析文本的特点，同时猜测**从交互历史中找到少量电影，刚好涵盖或者和标签非常接近**这个内容，7b模型不容易学习到。