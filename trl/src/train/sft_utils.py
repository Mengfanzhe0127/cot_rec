import warnings
from typing import Union, List, Optional, Any, Dict

import numpy as np
from transformers import PreTrainedTokenizerBase
from transformers.data.data_collator import DataCollatorMixin, pad_without_fast_tokenizer_warning


def get_response_template_ids(model_name_or_path, tokenizer, response_template=None):
    if response_template is None:
        raise

    response_template = '\n' + response_template
    if 'meta-llama/Llama-3.1-8B-Instruct' in model_name_or_path:
        response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)[1:]
    elif 'Microsoft_Phi-4' in model_name_or_path:
        response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)
    elif 'Qwen/Qwen2.5-7B-Instruct' in model_name_or_path or 'Qwen/Qwen2.5-7B' in model_name_or_path:
        response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)[1:] # 去除'\n'
    else:
        raise
    return response_template_ids

def get_instruction_template_ids(model_name_or_path, tokenizer, instruction_template=None):
    if instruction_template is None:
        raise

    instruction_template = '\n' + instruction_template
    if 'meta-llama/Llama-3.1-8B-Instruct' in model_name_or_path:
        instruction_template_ids = tokenizer.encode(instruction_template, add_special_tokens=False)[1:]
    elif 'Microsoft_Phi-4' in model_name_or_path:
        instruction_template_ids = tokenizer.encode(instruction_template, add_special_tokens=False)
    elif 'Qwen/Qwen2.5-7B-Instruct' in model_name_or_path or 'Qwen/Qwen2.5-7B' in model_name_or_path:
        instruction_template_ids = tokenizer.encode(instruction_template, add_special_tokens=False)[1:]
    else:
        raise
    return instruction_template_ids


class DataCollatorForCompletionOnlyLM(DataCollatorMixin):
    def __init__(
        self,
        tokenizer: PreTrainedTokenizerBase,
        response_template: Union[str, List[int]],
        instruction_template: Optional[Union[str, List[int]]] = None,
        ignore_index: int = -100,
        return_tensors: str = "pt",
        pad_to_multiple_of: Optional[int] = None,
        padding=False,
        max_length=None,
    ):
        self.tokenizer = tokenizer
        self.return_tensors = return_tensors
        self.pad_to_multiple_of = pad_to_multiple_of
        self.padding = padding
        self.max_length = max_length

        self.instruction_template = instruction_template
        if isinstance(instruction_template, str):
            # The user provides a string, must tokenize
            self.instruction_token_ids = self.tokenizer.encode(self.instruction_template, add_special_tokens=False)
        else:
            # The user already provides the token ids
            self.instruction_token_ids = instruction_template # 传递template ids

        self.response_template = response_template
        if isinstance(response_template, str):
            # The user provides a string, must tokenize
            self.response_token_ids = self.tokenizer.encode(self.response_template, add_special_tokens=False)
        else:
            # The user already provides the token ids
            self.response_token_ids = response_template # 传递template ids，目前使用

        if self.instruction_template and self.tokenizer.pad_token_id == self.tokenizer.eos_token_id:
            warnings.warn(
                "The pad_token_id and eos_token_id values of this tokenizer are identical. "
                "If you are planning for multi-turn training, "
                "it can result in the model continuously generating questions and answers without eos token. "
                "To avoid this, set the pad_token_id to a different value."
            )

        self.ignore_index = ignore_index

    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:
        # handle attrs in addition to what the pad method needs
        
        
        batch = pad_without_fast_tokenizer_warning(
            self.tokenizer, 
            examples,
            return_tensors="pt",
            pad_to_multiple_of=self.pad_to_multiple_of, 
            padding=self.padding, 
            max_length=self.max_length,
            # truncation=True
        ) # 对输入进行填充，不指定max_length下默认填充到最长值

        labels = batch["input_ids"].clone()
        if self.tokenizer.pad_token_id is not None:
            labels[labels == self.tokenizer.pad_token_id] = self.ignore_index
        batch["labels"] = labels

        # labels与input_ids相同，只是将pad_token_id替换为ignore_index
        # batch labels用于计算损失

        if self.instruction_template is None:
            for i in range(len(examples)):
                response_token_ids_start_idx = None

                for idx in np.where(batch["labels"][i] == self.response_token_ids[0])[0]:
                    # `response_token_ids` is `'### Response:\n'`, here we are just making sure that the token IDs match
                    if (
                        self.response_token_ids
                        == batch["labels"][i][idx : idx + len(self.response_token_ids)].tolist()
                    ):
                        response_token_ids_start_idx = idx # 标准response template的起始位置

                if response_token_ids_start_idx is None:
                    warnings.warn(
                        f"Could not find response key `{self.response_template}` in the "
                        f'following instance: {self.tokenizer.decode(batch["input_ids"][i])} '
                        f"This instance will be ignored in loss calculation. "
                        f"Note, if this happens often, consider increasing the `max_seq_length`."
                    )
                    batch["labels"][i, :] = self.ignore_index
                else:
                    response_token_ids_end_idx = response_token_ids_start_idx + len(self.response_token_ids)

                    # Make pytorch loss function ignore all tokens up through the end of the response key
                    batch["labels"][i, :response_token_ids_end_idx] = self.ignore_index # 在无instruction template下，忽略response template之前的所有token

        else:
            for i in range(len(examples)):
                response_token_ids_idxs = []
                human_token_ids_idxs = []

                for assistant_idx in np.where(batch["labels"][i] == self.response_token_ids[0])[0]:
                    # find the indexes of the start of a response.
                    if (
                        self.response_token_ids
                        == batch["labels"][i][assistant_idx : assistant_idx + len(self.response_token_ids)].tolist()
                    ):
                        response_token_ids_idxs.append(assistant_idx + len(self.response_token_ids))

                if len(response_token_ids_idxs) == 0:
                    warnings.warn(
                        f"Could not find response key `{self.response_template}` in the "
                        f'following instance: {self.tokenizer.decode(batch["input_ids"][i])} '
                        f"This instance will be ignored in loss calculation. "
                        f"Note, if this happens often, consider increasing the `max_seq_length`."
                    )
                    batch["labels"][i, :] = self.ignore_index

                human_token_ids = self.instruction_token_ids
                for human_idx in np.where(batch["labels"][i] == human_token_ids[0])[0]:
                    # find the indexes of the start of a human answer.
                    if human_token_ids == batch["labels"][i][human_idx : human_idx + len(human_token_ids)].tolist():
                        human_token_ids_idxs.append(human_idx)

                if len(human_token_ids_idxs) == 0:
                    warnings.warn(
                        f"Could not find instruction key `{self.instruction_template}` in the "
                        f'following instance: {self.tokenizer.decode(batch["input_ids"][i])} '
                        f"This instance will be ignored in loss calculation. "
                        f"Note, if this happens often, consider increasing the `max_seq_length`."
                    )
                    batch["labels"][i, :] = self.ignore_index

                if (
                    len(human_token_ids_idxs) > 0
                    and len(response_token_ids_idxs) > 0
                    and human_token_ids_idxs[0] > response_token_ids_idxs[0]
                ):
                    human_token_ids_idxs = [0] + human_token_ids_idxs # [0, h1, h2]

                for idx, (start, end) in enumerate(zip(human_token_ids_idxs, response_token_ids_idxs)): # 配对的形式设置ignore_index，第一个配对删除system部分
                    # Make pytorch loss function ignore all non response tokens
                    if idx != 0:
                        batch["labels"][i, start:end] = self.ignore_index
                    else:
                        batch["labels"][i, :end] = self.ignore_index

                if len(response_token_ids_idxs) < len(human_token_ids_idxs):
                    batch["labels"][i, human_token_ids_idxs[-1] :] = self.ignore_index
        
        # warnings.warn(f"batch: {batch}")

        return batch
    